#!/usr/bin/env python3

import argparse
import Bio.Seq
import Bio.SeqIO
import Bio.SeqRecord
import datetime
import gemmi
import gzip
import models
import os
import random
import rcsb
import seqtools
import sys
import tasks
import urllib.request
import utils
import xml.etree.ElementTree as ET

## ARGUMENTS

def add_required_arguments(parser):
  required = parser.add_argument_group("Required arguments")
  required.add_argument("--pdb-coords", metavar="DIR", required=True, help="Directory containing divided coordinate files in gzipped PDB format (??/pdb????.ent.gz)")
  required.add_argument("--pdb-sfs", metavar="DIR", required=True, help="Directory containing divided reflection data files in gzipped CIF format (??/r????sf.ent.gz)")
  required.add_argument("--pdb-reports", metavar="DIR", required=True, help="Directory containing divided validation reports in gzipped XML format (??/????/????_validation.xml.gz)")
  required.add_argument("--gesamt-archive", metavar="DIR", required=True, help="Directory containing a GESAMT archive to search for structural homologues")

def add_optional_arguments(parser):
  optional = parser.add_argument_group("Optional arguments")
  optional.add_argument("--help", action="help", help="Show this help message and exit")
  optional.add_argument("--jobs", metavar="N", default="auto", help="Number of CPU threads to use (default: auto)")
  optional.add_argument("--model-model-rmsd", type=float, metavar="X", default=1.5, help="Minimum RMSD between two models (default: 1.5)")
  optional.add_argument("--model-model-seqid", type=int, metavar="[95,90,70,50,40,30]", choices=[95,90,70,50,40,30], default=95, help="Maximum sequence identity between two models (default: 95)")
  optional.add_argument("--model-target-qscore", type=float, metavar="X", default=0.1, help="Minimum GESAMT Q-score between model and target (default: 0.1)")
  optional.add_argument("--model-target-rmsd", type=float, metavar="X", default=3.0, help="Maximum RMSD between model and target (default: 3.0)")
  optional.add_argument("--model-target-seqid", type=int, metavar="[95,90,70,50,40,30]", choices=[95,90,70,50,40,30], default=95, help="Maximum sequence identity between model and target (default: 95)")
  optional.add_argument("--num-models", type=int, metavar="N", default=10, help="Maximum number of models to choose for each unique chain (default: 10)")
  optional.add_argument("--num-structures", type=int, metavar="N", default=5, help="Maximum number of structures to find in each resolution bin (default: 200)") # TODO: Change back to default
  optional.add_argument("--res-bins", type=int, metavar="N", default=10, help="Number of resolution bins (default: 10)")
  optional.add_argument("--res-max", type=float, metavar="X", default=3.5, help="Maximum resolution (exclusive) (default: 3.5)")
  optional.add_argument("--res-min", type=float, metavar="X", default=1.0, help="Minimum resolution (inclusive) (default: 1.0)")
  optional.add_argument("--structure-seqid", type=int, metavar="[95,90,70,50,40,30]", choices=[95,90,70,50,40,30], default=50, help="Maximum sequence identity used for filtering structures with similar chains (default: 50)")
  optional.add_argument("--validation-clash", type=int, metavar="N", default=40, help="Clashscore percentile threshold (default: 40)")
  optional.add_argument("--validation-rama", type=int, metavar="N", default=40, help="Percentage Ramachandran outliers percentile threshold (default: 40)")
  optional.add_argument("--validation-rfree", type=int, metavar="N", default=50, help="Rfree percentile threshold (default: 50)")
  optional.add_argument("--validation-rota", type=int, metavar="N", default=40, help="Percentage rotamer outliers percentile threshold (default: 40)")
  optional.add_argument("--validation-rsrz", type=int, metavar="N", default=40, help="Percentage RSRZ outliers percentile threshold (default: 40)")

def add_calculated_arguments(args):
  args.res_step = (args.res_max - args.res_min) / args.res_bins
  args.jobs = os.cpu_count() if args.jobs == "auto" else int(args.jobs)
  args.jobs = max(1, min(args.jobs, os.cpu_count()))

def parse_args():
  description = "Creates a new molecular replacement test set."
  parser = argparse.ArgumentParser(description=description, add_help=False)
  add_required_arguments(parser)
  add_optional_arguments(parser)
  args = parser.parse_args()
  for arg in sorted(vars(args)):
    user_arg = "--%s" % arg.replace("_", "-")
    print("%-21s  %s" % (user_arg, getattr(args, arg)))
  print("")
  add_calculated_arguments(args)
  return args

## CHOOSE STRUCTURES

class ResolutionBin:
  def __init__(self, i):
    self.min_res = args.res_min + i * args.res_step
    self.max_res = args.res_min + (i + 1) * args.res_step
    self.structures = []
    self.chosen = []

def assign_resolution_bins(structures):
  bins = [ResolutionBin(i) for i in range(args.res_bins)]
  for structure in structures.values():
    if (structure.resolution < args.res_min or
        structure.resolution >= args.res_max):
      continue
    i = int((structure.resolution - args.res_min) / args.res_step)
    bins[i].structures.append(structure)
  return bins

def gzipped_coords(structure):
  pdb = structure.id.lower()
  return os.path.join(args.pdb_coords, pdb[1:3], "pdb%s.ent.gz" % pdb)

def gzipped_sfs(structure):
  pdb = structure.id.lower()
  return os.path.join(args.pdb_sfs, pdb[1:3], "r%ssf.ent.gz" % pdb)

def gzipped_report(structure):
  pdb = structure.id.lower()
  return os.path.join(args.pdb_reports, pdb[1:3], pdb, "%s_validation.xml.gz" % pdb)

def input_files_exist(structure):
  all(os.path.exists(path) for path in {
    gzipped_coords(structure),
    gzipped_sfs(structure),
    gzipped_report(structure),
  })

def validation_report_okay(structure):
  attrib_key_dict = {
    "relative-percentile-DCC_Rfree": "validation_clash",
    "relative-percentile-clashscore": "validation_rama",
    "relative-percentile-percent-RSRZ-outliers": "validation_rfree",
    "relative-percentile-percent-rama-outliers": "validation_rota",
    "relative-percentile-percent-rota-outliers": "validation_rsrz",
  }
  path = gzipped_report(structure.id.lower())
  if not os.path.exists(path): return False
  with gzip.open(path) as f:
    content = f.read()
  attribs = ET.fromstring(content).find("Entry").attrib
  for attrib in attrib_key_dict:
    key = attrib_key_dict[attrib]
    if attrib not in attribs: return False
    percentile = float(attribs[attrib])
    threshold = getattr(args, key)
    if percentile < threshold: return False
    setattr(structure, key, percentile)
  return True

def choose_structures(structures):
  print("## CHOOSING STRUCTURES ##\n")
  res_bins = assign_resolution_bins(structures)
  chosen_clusters = set()
  cluster_attr = "cluster%d" % args.structure_seqid
  res_bins.sort(key=lambda res_bin: len(res_bin.structures))
  for res_bin in res_bins:
    title = "Choosing %.2f-%.2fA structures (%d to choose from)" % (
      res_bin.min_res, res_bin.max_res, len(res_bin.structures))
    progress_bar = utils.ProgressBar(title, args.num_structures)
    random.shuffle(res_bin.structures)
    num_checked = 0
    num_missing_files = 0
    num_too_similar = 0
    num_failed_validation = 0
    for structure in res_bin.structures:
      passed = True
      num_checked += 1
      if not input_files_exist(structure):
        num_missing_files += 1
        passed = False
      clusters = {getattr(c, cluster_attr) for c in structure.chains.values()}
      if any(c in chosen_clusters for c in clusters):
        num_too_similar += 1
        passed = False
      if not validation_report_okay(structure):
        num_failed_validation += 1
        passed = False
      if passed:
        res_bin.chosen.append(structure)
        chosen_clusters.update(clusters)
        progress_bar.increment()
        if len(res_bin.chosen) == args.num_structures:
          break
    progress_bar.finish()
    print("Total number checked:          %6d" % num_checked)
    print("Missing input files:           %6d" % num_missing_files)
    print("Too similar to already chosen: %6d" % num_too_similar)
    print("Failed validation checks:      %6d" % num_failed_validation)
    print("")
  return {s.id: s for r in res_bins for s in r.chosen}

## GET SEQUENCES

def download_sequences(structures):
  print("Downloading sequences ...")
  ids = [s.id for s in structures.values()]
  url = "https://www.rcsb.org/pdb/download/downloadFastaFiles.do"
  url += "?structureIdList=%s" % ",".join(ids)
  url += "&compressionType=uncompressed"
  urllib.request.urlretrieve(url, "sequences.fasta")

def extract_sequences(structures):
  print("Extracting sequences ...")
  for record in Bio.SeqIO.parse("sequences.fasta", "fasta"):
    structureId = record.id[:4]
    chainId = record.id[5:6]
    if structureId in structures:
      structure = structures[structureId]
      if chainId in structure.chains:
        chain = structure.chains[chainId]
        chain.seq = str(record.seq)

def write_sequence(structure, path):
  records = []
  for chainId in structure.chains:
    chain = structure.chains[chainId]
    record = Bio.SeqRecord.SeqRecord(Bio.Seq.Seq(chain.seq),
      id="%s:%s" % (structure.id, chainId), description="")
    records.append(record)
  Bio.SeqIO.write(records, path, "fasta")
  return structure

def write_deposited_sequence(key, structure):
  structure = write_sequence(structure, structure.path("deposited.fasta"))
  return key, structure

def remove_duplicate_chains(key, structure):
  seq_chain_dict = {}
  for chainId in sorted(structure.chains):
    chain = structure.chains[chainId]
    if chain.seq not in seq_chain_dict:
      chain.copies = 1
      seq_chain_dict[chain.seq] = chain
    else:
      seq_chain_dict[chain.seq].copies += 1
      del structure.chains[chainId]
  return key, structure

def write_unique_sequence(key, structure):
  structure = write_sequence(structure, structure.path("unique.fasta"))
  return key, structure

def add_chains_metadata(key, structure):
  chains = {}
  for chainId in sorted(structure.chains):
    chain = structure.chains[chainId]
    chains[chainId] = {
      "length": len(chain.seq),
      "copies": chain.copies
    }
  structure.add_metadata("chains", chains)
  return key, structure

def get_sequences(structures):
  print("## GETTING FULL SEQUENCES ##\n")
  download_sequences(structures)
  extract_sequences(structures)
  utils.parallel("Writing deposited sequences", write_deposited_sequence, structures, args.jobs)
  utils.parallel("Removing duplicate chains", remove_duplicate_chains, structures, args.jobs)
  utils.parallel("Writing unique sequences", write_unique_sequence, structures, args.jobs)
  utils.parallel("Writing metadata", add_chains_metadata, structures, args.jobs)
  print("")

## PREPARE STRUCTURE DATA

def unzip_input_files(key, structure):
  utils.run("gunzip", ["-c", gzipped_coords(structure), stdout=structure.path["deposited.pdb"])
  utils.run("gunzip", ["-c", gzipped_sfs(structure), stdout=structure.path["deposited.cif"])
  return key, structure

def convert_to_mtz(key, structure):
  hklin = structure.path("deposited.cif")
  prefix = os.path.join(structure.id, "cif2mtz")
  result = tasks.cif2mtz(hklin, prefix)
  structure.jobs["cif2mtz"] = result
  return key, structure

def convert_amplitudes(key, structure):
  hklin = structure.jobs["cif2mtz"]["hklout"]
  seqin = structure.path("deposited.fasta")
  prefix = os.path.join(structure.id, "ctruncate")
  result = tasks.convert_amplitudes(hklin, seqin, prefix)
  structure.jobs["ctruncate"] = result
  return key, structure

def add_freer_flag(key, structure):
  hklin = structure.jobs["ctruncate"]["hklout"]
  prefix = os.path.join(structure.id, "freerflag")
  result = tasks.add_freer_flag(hklin, prefix)
  structure.jobs["freerflag"] = result
  return key, structure

def rename_columns(key, structure):
  hklin = structure.jobs["freerflag"]["hklout"]
  colin = ["FreeR_flag"] + structure.jobs["ctruncate"]["colout"]
  colout = ["FREE", "FP", "SIGFP"]
  prefix = os.path.join(structure.id, "cad")
  result = tasks.select_and_rename_columns(hklin, colin, colout, prefix)
  structure.jobs["cad"] = result
  return key, structure

def remove_unl_residues(key, structure):
  xyzin = structure.path("deposited.pdb")
  prefix = os.path.join(structure.id, "no_unl")
  result = tasks.remove_unl_residues(xyzin, prefix)
  structure.jobs["no_unl"] = result
  return key, structure

def refine_deposited_structure(key, structure):
  hklin = structure.jobs["cad"]["hklout"]
  xyzin = structure.jobs["no_unl"]["xyzout"]
  prefix = os.path.join(structure.id, "refmac")
  result = tasks.refine(hklin, xyzin, prefix)
  structure.jobs["refmac"] = result
  return key, structure

def add_structure_metadata(key, structure):
  structure.add_metadata("refined_rwork", structure.jobs["refmac"]["final_rwork"])
  structure.add_metadata("refined_rfree", structure.jobs["refmac"]["final_rfree"])
  pdbin = structure.path("deposited_pdb")
  structure.add_metadata("semet", utils.is_semet(pdbin))
  mtzin = structure.jobs["cad"]["hklout"]
  mtz = gemmi.read_mtz_file(mtzin)
  structure.add_metadata("spacegroup", mtz.spacegroup.hm)
  structure.add_metadata("resolution", round(mtz.resolution_high(), 2))
  structure.add_metadata("asu_volume", round(mtz.cell.volume / mtz.nsymop))
  return key, structure

def prepare_structure_data(structures):
  print("## PREPARING STRUCTURE DATA ##\n")
  steps = [
    ("Unzipping input files", unzip_input_files),
    ("Converting CIF files to MTZ", convert_to_mtz),
    ("Converting amplitudes", convert_amplitudes),
    ("Adding free-R flags", add_freer_flag),
    ("Renaming columns", rename_columns),
    ("Removing UNL residues", remove_unl_residues),
    ("Refining structures", refine_deposited_structure),
    ("Adding metadata", add_structure_metadata),
  ]
  for title, func in steps:
    utils.parallel(title, func, structures, args.jobs)
    utils.remove_errors(structures)
  print("")

## FIND HOMOLOGUES

def search_for_homologues(chains):
  progress_bar = utils.ProgressBar("Performing gesamt searches", len(chains))
  for chain in chains.values():
    xyzin = chain.structure.jobs["refmac"]["xyzout"]
    prefix = os.path.join(structure.id, "chain%s_gesamt" % chain.id)
    result = tasks.structural_homologues(xyzin, chain.id, prefix, args.gesamt_archive, args.jobs)
    chain.jobs["gesamt"] = result
    progress_bar.increment()
  print("")

class Hit:
  def __init__(self, target, pdb, chain, qscore, rmsd, seqid):
    self.target = target
    self.pdb = pdb
    self.chain = chain
    self.qscore = qscore
    self.rmsd = rmsd
    self.seqid = seqid
    self.cluster = None
    if pdb.upper() in structures:
      for chain in structures[pdb.upper()].chains:
        if chain.id == self.chain:
          attr = "cluster%d" % args.diff_seqid
          self.cluster = getattr(chain, attr)
          break

def possible_gesamt_hits(chain):
  hits_path = os.path.join(args.pdb, "chain%s_gesamt.txt" % chain)
  with open(hits_path) as hits_file:
    for line in hits_file:
      if line[0] == "#": continue
      split = line.split()
      hit_pdb = split[1].lower()
      hit_chain = split[2]
      qscore = float(split[3])
      rmsd = float(split[4])
      seqid = float(split[5])
      if qscore > args.min_qscore and rmsd < args.max_rmsd and seqid < (args.diff_seqid / 100.0):
        yield Hit(hit_pdb, hit_chain, qscore, rmsd, seqid)

def alignment_stats(hit1, hit2):
  tmp_prefix = "tmp%s" % uuid.uuid4()
  path1 = os.path.join(args.pdbs, hit1.pdb[1:3], "pdb%s.ent.gz" % hit1.pdb)
  path2 = os.path.join(args.pdbs, hit2.pdb[1:3], "pdb%s.ent.gz" % hit2.pdb)
  tasks.superpose(path1, hit1.chain, path2, hit2.chain, tmp_prefix)
  os.remove(tmp_path) # TODO: Remove all files starting with this prefix
  return stats

def should_choose(hit, chosen_hits):
  if hit.cluster is None: return False
  for chosen_hit in reversed(chosen_hits):
    if hit.cluster == chosen_hit.cluster: return False
  for chosen_hit in reversed(chosen_hits):
    stats = alignment_stats(hit, chosen_hit)
    if stats["rmsd"] is not None and stats["rmsd"] < args.diff_rmsd: return False
    if stats["seqid"] is not None and stats["seqid"] > (args.diff_seqid / 100.0): return False
  return True

def choose_hits(key, chain):
  chain.chosen_hits = []
  for hit in possible_gesamt_hits(chain):
    if should_choose(hit, chain.chosen_hits):
      chain.chosen_hits.append(hit)
      os.mkdir(os.path.join(args.pdb, "models", "%s_%s%s" % (chain, hit.pdb, hit.chain)))
      if len(chain.chosen_hits) == args.max_models:
        break
  return key, chain

def find_homologues(structures):
  print("## FINDING HOMOLOGUES TO MAKE MR MODELS FROM ##\n")
  # chains = {}
  # for structure in structures.values():
  #   os.mkdir(os.path.join(structure.id, "models"))
  #   for chain in structure.chains.values():
  #     key = "%s:%s" % (structure.id, chain.id)
  #     chains[key] = chain
  # search_for_homologues(chains)
  # utils.parallel("Choosing from gesamt results", choose_hits, chains, args.jobs)
  print("")

## MR

def mr(structures):
  print("## PREPARING MODELS AND PERFORMING MOLECULAR REPLACEMENT ##\n")
  # tasks.superpose(target_gz(), args.chain, model_gz(), args.model_chain, os.path.join(args.dir, "gesamt"))
  # tasks.post_process_gesamt_alignment(
  #   alnin=gesamt_fasta(),
  #   alnout=sculptor_fasta(),
  #   id1="%s%s" % (args.pdb, args.chain),
  #   id2="%s%s" % (args.model_pdb, args.model_chain),
  # )
  # tasks.trim_model(model_gz(), args.model_chain, sculptor_fasta(), os.path.join(args.dir, "sculptor"))
  # tasks.mr(joined_mtz(), sculptor_pdb(), identity(), os.path.join(args.dir, "phaser"), n_copies(), pdbtools.count_elements())
  # tasks.refine(joined_mtz, phaser_pdb, os.path.join(args.dir, "refmac"))
  # tasks.combine_mtz(cmtzjoin_mtz(), [
  #   (joined_mtz(), "FP,SIGFP", "FP,SIGFP"),
  #   (joined_mtz(), "FREE", "FREE"),
  #   (joined_mtz(), "reference.HLA,reference.HLB,reference.HLC,reference.HLD", "reference.HLA,reference.HLB,reference.HLC,reference.HLD"),
  #   (refmac_mtz(), "HLACOMB,HLBCOMB,HLCCOMB,HLDCOMB", "model.HLA,model.HLB,model.HLC,model.HLD"),
  # ])
  # tasks.compare_phases(cmtzjoin_mtz(), "FP,SIGFP", "model.HLA,model.HLB,model.HLC,model.HLD", "reference.HLA,reference.HLB,reference.HLC,reference.HLD")
  print("")

## MAIN

if __name__ == "__main__":
  print("###################")
  print("## Create MR Set ##")
  print("###################")
  print("")
  print("Please cite:")
  print("XXXX")
  print("")
  print("Time: %s\n" % datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"))

  if "CCP4" not in os.environ:
    sys.exit("Please setup the CCP4 environment")

  args = parse_args()
  rcsb_structures = rcsb.structures()
  structures = choose_structures(rcsb_structures)
  if len(structures) < 1: sys.exit("ERROR: No structures chosen")
  # structures = { "1O6A": structures["1O6A"] }
  for structure in structures.values():
    os.mkdir(structure.id)
    # input_files_exist(structure)
  get_sequences(structures)
  prepare_structure_data(structures)

  # import pickle
  # with open("structures.pkl", "wb") as f:
  #   pickle.dump(structures, f)
  # with open("structures.pkl", "rb") as f:
  #   structures = pickle.load(f)

  find_homologues(structures)

  print("------------------")
  print("Normal termination")
  print("------------------")
