#!/usr/bin/python

from Bio import SeqIO, Seq
from Bio.Blast import NCBIXML, NCBIWWW
import csv
import datetime
import gzip
import os
import random
import re
import shutil
import subprocess
import sys
import time
import urllib
import urllib2
import urlparse
import xml.etree.ElementTree as ET


def hit_dir(hit): return "%s%s-%s%s" % (hit["query_pdb"], hit["query_chain"], hit["match_pdb"], hit["match_chain"])
def alignment_aln(hit, domain): return "%s/alignment%s.aln" % (hit_dir(hit), domain["id"])
def deposited_cif(pdb): return "%s-deposited.cif" % pdb
def deposited_mtz(pdb): return "%s-deposited.mtz" % pdb
def deposited_pdb(pdb): return "%s-deposited.pdb" % pdb
def gesamt_log(hit, domain, chain): return "%s/gesamt%s%s.log" % (hit_dir(hit), domain["id"], chain)
def gesamt_pdb(hit, domain, chain): return "%s/gesamt%s%s.pdb" % (hit_dir(hit), domain["id"], chain)
def match_cif(hit): return "%s/%s.cif" % (hit_dir(hit), hit["match_pdb"])
def match_pdb(hit): return "%s/%s.pdb" % (hit_dir(hit), hit["match_pdb"])
def match_chain(hit): return "%s/%s%s.pdb" % (hit_dir(hit), hit["match_pdb"], hit["match_chain"])
def model_pdb(hit, domain): return "%s/model%s.pdb" % (hit_dir(hit), domain["id"])
def model_refmac_log(hit, domain): return "%s/model%s-refmac.log" % (hit_dir(hit), domain["id"])
def model_refmac_mtz(hit, domain): return "%s/model%s-refmac.mtz" % (hit_dir(hit), domain["id"])
def model_refmac_pdb(hit, domain): return "%s/model%s-refmac.pdb" % (hit_dir(hit), domain["id"])
def phmmer_xml(pdb, chain): return "%s%s-phmmer.xml" % (pdb, chain)
def protein_fasta(pdb): return "%s-protein.fasta" % pdb
def protein_mtz(pdb): return "%s-protein.mtz" % pdb
def protein_pdb(pdb): return "%s-protein.pdb" % pdb
def protein_refmac_log(pdb): return "%s-protein-refmac.log" % pdb
def protein_refmac_mtz(pdb): return "%s-protein-refmac.mtz" % pdb
def protein_refmac_pdb(pdb): return "%s-protein-refmac.pdb" % pdb
def refmac_log(pdb): return "%s-refmac.log" % pdb
def refmac_mtz(pdb): return "%s-refmac.mtz" % pdb
def refmac_pdb(pdb): return "%s-refmac.pdb" % pdb
def sculptor_pdb(hit, domain): return "%s/sculptor%s_%s%s.pdb" % (hit_dir(hit), domain["id"], hit["match_pdb"], hit["match_chain"])
def sculptor_log(hit, domain): return "%s/sculptor%s.log" % (hit_dir(hit), domain["id"])
def unique_fasta(pdb): return "%s-unique.fasta" % pdb
def validation_xml(pdb): return "%s-validation.xml" % pdb


def run(command, stdin=[], logfile=None):
  pstdin = subprocess.PIPE if len(stdin) > 0 else None
  pstdout = subprocess.PIPE if logfile is None else open(logfile, 'w')
  command = command.split(' ')
  p = subprocess.Popen(command, stdin=pstdin, stdout=pstdout)
  if pstdin == subprocess.PIPE:
    for line in stdin:
      p.stdin.write(line + '\n')
    p.stdin.close()
  p.wait()


def log(message):
  print message
  sys.stdout.flush()


def change_dir(pdb=None):
  # base = "/y/people/pb555/phd/runs/20180125_1"
  base = "/scratch/pb555/runs/20180406_1_mrtestset"
  os.chdir(base)
  if pdb is not None:
    if not os.path.exists(pdb):
      os.mkdir(pdb)
    os.chdir(pdb)


def download(url, destination):
  if os.path.exists(destination): return
  tmpname = os.path.basename(urlparse.urlparse(url).path)
  urllib.urlretrieve(url, tmpname)
  if tmpname[-3:] == ".gz":
    subprocess.call(["gunzip", "-f", tmpname])
    tmpname = tmpname[:-3]
  os.rename(tmpname, destination)


def read(filename):
  if filename[-3:] == ".gz":
    with gzip.open(filename) as f:
      return f.read()
  else:
    with open(filename) as f:
      return f.read()


def count_atoms(pdbin):
  command = ["grep", "-c", "^ATOM  ", pdbin]
  p = subprocess.Popen(command, stdout=subprocess.PIPE)
  return int(p.stdout.read())


def cextractprotein(xyzin, xyzout, logfile = None):
  # command = "/y/people/pb555/phd/programs/cextractprotein/cextractprotein"
  command = "/scratch/pb555/programs/cextractprotein/cextractprotein"
  command += " -xyzin " + xyzin
  command += " -xyzout " + xyzout
  command += " -min-length 6"
  command += " -truncate-unk"
  run(command, [], logfile)


def chainsaw(xyzin, alignin, xyzout, logfile=None):
  command = "chainsaw"
  command += " xyzin " + xyzin
  command += " alignin " + alignin
  command += " xyzout " + xyzout
  stdin = ["END"]
  run(command, stdin, logfile)


def cif2mtz(hklin, hklout, logfile = None):
  command = "cif2mtz hklin %s hklout %s" % (hklin, hklout)
  stdin = ["END"]
  run(command, stdin, logfile)


def cmodeltoseq(pdbin, seqout, logfile = None):
  # command = "/y/people/pb555/phd/programs/devtools/install/bin/cmodeltoseq"
  command = "/scratch/pb555/programs/cmodeltoseq/cmodeltoseq"
  command += " -pdbin " + pdbin
  command += " -seqout " + seqout
  command += " -include-unk"
  run(command, [], logfile)


def gesamt(xyzref, refchain, xyzwrk, xyzout, logfile=None):
  command = "gesamt %s -s /*/%s %s -high -o %s" % (xyzref, refchain, xyzwrk, xyzout)
  run(command, [], logfile)


def pdbcur(xyzin, xyzout, stdin):
  command = "pdbcur"
  command += " xyzin " + xyzin
  command += " xyzout " + xyzout
  run(command, stdin)


def pdb_merge(xyzin1, xyzin2, xyzout):
  command = "pdb_merge xyzin1 %s xyzin2 %s xyzout %s" % (xyzin1, xyzin2, xyzout)
  stdin = ["NOMERGE", "END"]
  run(command, stdin)


def refmac(hklin, xyzin, hklout, xyzout, cycles, logfile = None):
  command = "refmac5"
  command += " HKLIN " + hklin + " HKLOUT " + hklout
  command += " XYZIN " + xyzin + " XYZOUT " + xyzout
  stdin = [
    "NCYCLES %d" % cycles,
    "WEIGHT AUTO",
    "MAKE HYDR ALL",
    "REFI BREF ISOT",
    "MAKE NEWLIGAND EXIT",
    "SCALE TYPE SIMPLE",
    "SOLVENT YES",
    "PHOUT",
    "END"
  ]
  run(command, stdin, logfile)


def sculptor(xyzin, alignin, folder, domain, logfile=None):
  command = "phaser.sculptor --stdin"
  stdin = [
    "input {",
    "model {"
    "file_name = %s" % xyzin,
    "remove_alternate_conformations = True"
    "}",
    "alignment {",
    "file_name = %s" % alignin,
    "target_index = 1",
    "}",
    "}",
    "output {",
    "folder = %s" % folder,
    "root = 'sculptor%s'" % domain["id"],
    "}",
    "macromolecule {",
    "pruning {",
    "use = schwarzenbacher",
    "}",
    "bfactor {",
    "use = asa",
    "}",
    "}"
  ]
  run(command, stdin, logfile)


def superpose(xyzwrk, xyzref, refchain, xyzout, logfile=None):
  command = "superpose %s %s -s %s -o %s" % (xyzwrk, xyzref, refchain, xyzout)
  run(command, [], logfile)


def validation_okay(pdb):
  base = "http://ftp.ebi.ac.uk/pub/databases/pdb/validation_reports/"
  url = base + "%s/%s/%s_validation.xml.gz" % (pdb[1:3], pdb, pdb)
  try:
    download(url, validation_xml(pdb))
    content = read(validation_xml(pdb))
  except Exception, e:
    log("EXCEPTION for %s: %s" % (pdb, str(e)))
    with open('targets_failvalidation', 'a') as f: f.write("%s\n" % pdb)
    return False

  attribs = ET.fromstring(content).find("Entry").attrib
  def key_okay(key, threshold):
    return key in attribs and float(attribs[key]) >= threshold
  if not key_okay("relative-percentile-DCC_Rfree", 50):
    with open('targets_failvalidation', 'a') as f: f.write("%s\n" % pdb)
    return False
  if not key_okay("relative-percentile-clashscore", 25):
    with open('targets_failvalidation', 'a') as f: f.write("%s\n" % pdb)
    return False
  if not key_okay("relative-percentile-percent-RSRZ-outliers", 25):
    with open('targets_failvalidation', 'a') as f: f.write("%s\n" % pdb)
    return False
  if not key_okay("relative-percentile-percent-rama-outliers", 25):
    with open('targets_failvalidation', 'a') as f: f.write("%s\n" % pdb)
    return False
  if not key_okay("relative-percentile-percent-rota-outliers", 25):
    with open('targets_failvalidation', 'a') as f: f.write("%s\n" % pdb)
    return False
  return True


def data_okay(pdb, res, ref_limits):
  base = "ftp://ftp.wwpdb.org/pub/pdb/data/structures/all/"
  try:
    download(base + "pdb/pdb%s.ent.gz" % pdb, deposited_pdb(pdb))
    download(base + "structure_factors/r%ssf.ent.gz" % pdb, deposited_cif(pdb))

    cif2mtz(deposited_cif(pdb), deposited_mtz(pdb))
    refmac(deposited_mtz(pdb), deposited_pdb(pdb), refmac_mtz(pdb), refmac_pdb(pdb), 5, refmac_log(pdb))

  except Exception, e:
    log("EXCEPTION for %s: %s" % (pdb, str(e)))
    with open('targets_failrefinement', 'a') as f: f.write("%s\n" % pdb)
    return False

  with open(refmac_log(pdb)) as f:
    log_content = f.read()
  match = re.search(r"\$\$ Final results \$\$", log_content)
  if match is None:
    with open('targets_failrefinement', 'a') as f: f.write("%s\n" % pdb)
    return False
  r_work = re.search(r" +R factor +\d\.\d+ +(\d\.\d+)", log_content).group(1)
  r_free = re.search(r" +R free +\d\.\d+ +(\d\.\d+)", log_content).group(1)
  
  if float(r_work) > ref_limits[res]["r_work"]:
    with open('targets_failrefinement', 'a') as f: f.write("%s\n" % pdb)
    return False
  if float(r_free) > ref_limits[res]["r_free"]:
    with open('targets_failrefinement', 'a') as f: f.write("%s\n" % pdb)
    return False
  return True


def bin_pdbs_by_res(chains, resolutions):
  bins = { res: set() for res in resolutions }
  for chain in chains:
    res = round(chain["res"], 1)
    if res in bins:
      bins[res].add(chain["pdb"])
  return bins


def choose_targets(maximum, resolutions, existing = [], exclude = []):
  with open("run_files/refinement.csv") as f:
    reader = csv.DictReader(f, quoting=csv.QUOTE_NONNUMERIC)
    ref_limits = {row["res"]:row for row in reader}

  with open("run_files/chains.csv") as f:
    reader = csv.DictReader(f, quoting=csv.QUOTE_NONNUMERIC)
    chains = [row for row in reader]

  test_set = { res: set() for res in resolutions }

  log("Removing chains that have failed previously")
  chains = [chain for chain in chains if chain["pdb"] not in exclude]

  log("Removing chains that are similar to existing targets")
  for pdb in existing:
    res = round([chain["res"] for chain in chains if chain["pdb"] == pdb][0], 1)
    test_set[res].add(pdb)
    clusters = set([chain["clus40"] for chain in chains if chain["pdb"] == pdb])
    chains = [chain for chain in chains if chain["clus40"] not in clusters]

  while len(resolutions) > 0:

    to_place = bin_pdbs_by_res(chains, resolutions)
    res = min(to_place, key=lambda res: len(to_place[res]))

    if len(to_place[res]) == 0 or len(test_set[res]) == maximum:
      maximum = min(maximum, len(test_set[res]))
      resolutions.remove(res)
      continue

    log("Choosing %s targets from resolution %s with %s files" % (maximum-len(test_set[res]), res, len(to_place[res])))

    found = False
    while len(to_place[res]) > 0:
      pdb = random.choice(tuple(to_place[res]))
      change_dir(pdb)
      log("Trying %s" % pdb)

      if validation_okay(pdb) and data_okay(pdb, res, ref_limits):
        found = True
        break
      else:
        chains = [chain for chain in chains if chain["pdb"] != pdb]
        to_place[res].remove(pdb)
        change_dir()
        shutil.rmtree(pdb)

    if found:
      log("Found target %s at resolution %s" % (pdb, res))
      test_set[res].add(pdb)
      clusters = set([chain["clus40"] for chain in chains if chain["pdb"] == pdb])
      chains = [chain for chain in chains if chain["clus40"] not in clusters]

  log('## Chosen Test Set ##')
  targets = []
  for res in test_set:
    log("%s %s" % (res, test_set[res]))
    targets.extend(test_set[res])
  return targets


def clusters_dict():
  with open("run_files/chains.csv") as f:
    reader = csv.DictReader(f, quoting=csv.QUOTE_NONNUMERIC)
    clusters = {(row["pdb"], row["chain"]): row["clus95"] for row in reader}
  return clusters


def extract_and_refine_protein(pdb):
  if not os.path.exists(protein_pdb(pdb)):
    pdbcur(refmac_pdb(pdb), "tmp.pdb", ["lvmodel /1, mostprob"])
    cextractprotein("tmp.pdb", protein_pdb(pdb))
    os.remove("tmp.pdb")
  if not os.path.exists(protein_fasta(pdb)):
    cmodeltoseq(protein_pdb(pdb), protein_fasta(pdb))
  if not os.path.exists(protein_refmac_log(pdb)):
    refmac(deposited_mtz(pdb), protein_pdb(pdb), protein_refmac_mtz(pdb), protein_refmac_pdb(pdb), 5, protein_refmac_log(pdb))


# Remove matches without an entry in the cluster dict (e.g. NMR, Cryo-EM)
# Remove matches in the same cluster as the query
# Remove matches in the same cluster as a previous match
def filter_similar_hits(hits, clusters):
  chosen = {}
  for hit in hits:
    query = (hit["query_pdb"], hit["query_chain"])
    match = (hit["match_pdb"], hit["match_chain"])
    if match not in clusters: continue
    if clusters[query] == clusters[match]: continue
    if (query, clusters[match]) in chosen: continue
    chosen[(query, clusters[match])] = hit
  return [chosen[key] for key in chosen]


def phmmer_pending(pdb, chain):
  root = ET.parse(phmmer_xml(pdb, chain))
  data = root.find("data")
  return "status" in data.attrib and data.attrib["status"] == "PEND"


def phmmer_search(pdb):
  url = "https://www.ebi.ac.uk/Tools/hmmer/search/phmmer"
  for record in SeqIO.parse(unique_fasta(pdb), "fasta"):
    if not os.path.exists(phmmer_xml(pdb, record.id)):
      parameters = {
        "seqdb": "pdb",
        "seq": str(record.seq)
      }
      enc_params = urllib.urlencode(parameters)
      request = urllib2.Request(url, enc_params, {"Accept": "text/xml"})
      for _ in range(60):
        data = urllib2.urlopen(request)
        with open(phmmer_xml(pdb, record.id), 'w') as f:
          f.write(data.read())
        pending = phmmer_pending(pdb, record.id)
        if not pending: break
        time.sleep(10)


# Only return included hit domains with non-identical sequences
def parse_phmmer(pdb, chains):
  hits = []
  for chain in chains:
    used_seqs = set()
    root = ET.parse(phmmer_xml(pdb, chain))
    for hit_element in root.find("data").findall("hits"):
      hit = {
        "query_pdb": pdb,
        "query_chain": chain,
        "match_pdb": hit_element.attrib["acc"][:4],
        "match_chain": hit_element.attrib["acc"][-1],
        "domains": []
      }
      domains = hit_element.findall("domains")
      for i in range(len(domains)):
        if domains[i].attrib["is_included"] != "1": continue
        query_seq = domains[i].attrib["alimodel"].upper().replace('.', '-')
        match_seq = domains[i].attrib["aliaseq"].upper().replace('.', '-')
        if query_seq == match_seq: continue
        if match_seq in used_seqs: continue
        used_seqs.add(match_seq)
        hit["domains"].append({
          "id": i+1,
          "query_seq": query_seq,
          "match_seq": match_seq
        })
      if len(hit["domains"]) > 0:
        hits.append(hit)
  return hits


def download_match(hit):
  if os.path.exists(match_chain(hit)) and count_atoms(match_chain(hit)) > 0: return
  base = "ftp://ftp.wwpdb.org/pub/pdb/data/structures/all/"
  try:
    download(base + "pdb/pdb%s.ent.gz" % hit["match_pdb"], match_pdb(hit))
  except:
    download(base + "mmCIF/%s.cif.gz" % hit["match_pdb"], match_cif(hit))
  if   os.path.exists(match_pdb(hit)): downloaded = match_pdb(hit)
  elif os.path.exists(match_cif(hit)): downloaded = match_cif(hit)
  else: raise RuntimeError("Could not download match in PDB or CIF format")
  for _ in range(10):
    pdbcur(downloaded, match_chain(hit), [
      "lvchain /1/%s" % hit["match_chain"],
      "write PDB"
    ])
    if os.path.exists(match_chain(hit)) and count_atoms(match_chain(hit)) > 0: return
    time.sleep(10)
  raise RuntimeError("Individual chain could not be extracted using pdbcur")


def write_alignment(hit, domain):
  if os.path.exists(alignment_aln(hit, domain)): return
  sequences = [
    SeqIO.SeqRecord(Seq.Seq(domain["query_seq"]), hit["query_pdb"] + hit["query_chain"]),
    SeqIO.SeqRecord(Seq.Seq(domain["match_seq"]), hit["match_pdb"] + hit["match_chain"])
  ]
  SeqIO.write(sequences, alignment_aln(hit, domain), "clustal")


# Return a list of different chains with info about similar chains
def unique_chains(pdb, clusters):
  result = {}
  to_write = []
  for record in SeqIO.parse(protein_fasta(pdb), "fasta"):
    if (pdb, record.id) in clusters:
      if clusters[(pdb, record.id)] not in result:
        to_write.append(record)
        result[clusters[(pdb, record.id)]] = {
          "id": record.id,
          "similar": []
        }
      else:
        result[clusters[(pdb, record.id)]]["similar"].append(record.id)
  SeqIO.write(to_write, unique_fasta(pdb), "fasta")
  return {result[key]["id"]:result[key] for key in result}


# Superpose chainsawed model onto protein.pdb (all similar chains)
def make_model(hit, domain, chains):
  pdb = hit["query_pdb"]

  # Superimpose on first chain (query chain)
  chain = hit["query_chain"]
  sculptor(match_chain(hit), alignment_aln(hit, domain), hit_dir(hit), domain, sculptor_log(hit, domain))
  if count_atoms(sculptor_pdb(hit, domain)) == 0: raise RuntimeError("Sculptor failed to trim the match chain using the alignment.")
  gesamt(protein_refmac_pdb(pdb), chain, sculptor_pdb(hit, domain), gesamt_pdb(hit, domain, chain), gesamt_log(hit, domain, chain))
  if not os.path.exists(gesamt_pdb(hit, domain, chain)): raise RuntimeError("Gesamt could not superimpose the sculptor model.")
  pdbcur(gesamt_pdb(hit, domain, chain), model_pdb(hit, domain), ["lvmodel /2"])

  # Superimpose on all chains similar to query (and merge results)
  for chain in chains[hit["query_chain"]]["similar"]:
    gesamt(protein_refmac_pdb(pdb), chain, sculptor_pdb(hit, domain), gesamt_pdb(hit, domain, chain), gesamt_log(hit, domain, chain))
    if not os.path.exists(gesamt_pdb(hit, domain, chain)): raise RuntimeError("Gesamt could not superimpose the sculptor model.")
    pdbcur(gesamt_pdb(hit, domain, chain), "tmp.pdb", ["lvmodel /2"])
    pdb_merge(model_pdb(hit, domain), "tmp.pdb", "merged.pdb")
    os.rename("merged.pdb", model_pdb(hit, domain))
    os.remove("tmp.pdb")

  # Add CRYST line into model from protein.pdb
  with open(protein_pdb(pdb)) as f:
    model_with_cryst = [l for l in f if (l[:5]=="CRYST" or l[:5]=="SCALE")]
  with open(model_pdb(hit, domain)) as f:
    model_with_cryst.extend([l for l in f if l[:6]!="REMARK"])
  with open(model_pdb(hit, domain), 'w') as f:
    f.writelines(model_with_cryst)
  # Run refmac
  refmac(deposited_mtz(pdb), model_pdb(hit, domain), model_refmac_mtz(hit, domain), model_refmac_pdb(hit, domain), 10, model_refmac_log(hit, domain))


def choose_models(pdb, clusters):
  log("Choosing models for %s" % pdb)
  try:
    change_dir(pdb)
    extract_and_refine_protein(pdb)
    chains = unique_chains(pdb, clusters)
    phmmer_search(pdb)
    hits = parse_phmmer(pdb, chains)
    hits = filter_similar_hits(hits, clusters)
  except Exception, e:
    log("EXCEPTION for %s: %s" % (pdb, str(e)))
    return
  for hit in hits:
    try:
      if not os.path.exists(hit_dir(hit)): os.mkdir(hit_dir(hit))
      download_match(hit)
    except Exception, e:
      log("EXCEPTION for %s/%s: %s" % (pdb, hit_dir(hit), str(e)))
      continue
    for domain in hit["domains"]:
      try:
        if os.path.exists(model_refmac_pdb(hit, domain)): continue
        write_alignment(hit, domain)
        make_model(hit, domain, chains)
      except Exception, e:
        log("EXCEPTION for %s/%s/model%s: %s" % (pdb, hit_dir(hit), domain["id"], str(e)))
        continue
  log("Models chosen for %s" % pdb)


## MAIN

change_dir()
clusters = clusters_dict()
resolutions = {round(0.1 * x, 1) for x in range(10, 41)}
log("TIME: Initialised at %s" % datetime.datetime.now())

# existing = [line.strip() for line in open("targets")]
# exclude = []
# exclude.extend([line.strip() for line in open("targets_failvalidation")])
# exclude.extend([line.strip() for line in open("targets_failrefinement")])
# exclude.extend([line.strip() for line in open("targets_failalignments")])
# targets = choose_targets(10, resolutions, existing, exclude)
# log("TIME: Targets chosen at %s" % datetime.datetime.now())

target = sys.argv[1]
choose_models(target, clusters)
log("TIME: Finished at %s" % datetime.datetime.now())
